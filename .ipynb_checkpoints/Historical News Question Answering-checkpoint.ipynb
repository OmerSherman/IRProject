{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# Download and Inspect the Collection\n",
    "\n",
    "The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\n",
    "\n",
    "Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\n",
    "\n",
    "GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12366,
     "status": "ok",
     "timestamp": 1762962835550,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "4xBdfDsPYdLA",
    "outputId": "32103be7-1880-4ccb-ea70-6800e841dec1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1356  100  1356    0     0   6263      0 --:--:-- --:--:-- --:--:--  6336\n",
      "\n",
      "  0 71.5M    0 81920    0     0   270k      0  0:04:31 --:--:--  0:04:31  270k\n",
      " 20 71.5M   20 14.3M    0     0  11.1M      0  0:00:06  0:00:01  0:00:05 14.3M\n",
      " 43 71.5M   43 30.7M    0     0  13.4M      0  0:00:05  0:00:02  0:00:03 15.3M\n",
      " 62 71.5M   62 44.9M    0     0  13.6M      0  0:00:05  0:00:03  0:00:02 14.9M\n",
      " 82 71.5M   82 59.0M    0     0  13.7M      0  0:00:05  0:00:04  0:00:01 14.7M\n",
      "100 71.5M  100 71.5M    0     0  14.0M      0  0:00:05  0:00:05 --:--:-- 14.8M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1356  100  1356    0     0   4602      0 --:--:-- --:--:-- --:--:--  4627\n",
      "\n",
      "  0 1315M    0 6162k    0     0  5648k      0  0:03:58  0:00:01  0:03:57 5648k\n",
      "  1 1315M    1 20.3M    0     0  9988k      0  0:02:14  0:00:02  0:02:12 14.4M\n",
      "  2 1315M    2 34.7M    0     0  11.2M      0  0:01:56  0:00:03  0:01:53 14.4M\n",
      "  3 1315M    3 49.3M    0     0  11.8M      0  0:01:50  0:00:04  0:01:46 14.1M\n",
      "  4 1315M    4 65.6M    0     0  12.9M      0  0:01:41  0:00:05  0:01:36 14.9M\n",
      "  6 1315M    6 79.4M    0     0  13.0M      0  0:01:40  0:00:06  0:01:34 14.7M\n",
      "  7 1315M    7 96.5M    0     0  13.6M      0  0:01:36  0:00:07  0:01:29 15.2M\n",
      "  8 1315M    8  113M    0     0  13.9M      0  0:01:34  0:00:08  0:01:26 15.6M\n",
      "  9 1315M    9  129M    0     0  14.2M      0  0:01:32  0:00:09  0:01:23 16.1M\n",
      " 11 1315M   11  146M    0     0  14.5M      0  0:01:30  0:00:10  0:01:20 16.1M\n",
      " 12 1315M   12  160M    0     0  14.4M      0  0:01:31  0:00:11  0:01:20 16.0M\n",
      " 13 1315M   13  175M    0     0  14.5M      0  0:01:30  0:00:12  0:01:18 15.8M\n",
      " 14 1315M   14  193M    0     0  14.7M      0  0:01:28  0:00:13  0:01:15 16.0M\n",
      " 15 1315M   15  207M    0     0  14.7M      0  0:01:29  0:00:14  0:01:15 15.6M\n",
      " 16 1315M   16  223M    0     0  14.8M      0  0:01:28  0:00:15  0:01:13 15.3M\n",
      " 18 1315M   18  237M    0     0  14.7M      0  0:01:29  0:00:16  0:01:13 15.5M\n",
      " 19 1315M   19  252M    0     0  14.7M      0  0:01:29  0:00:17  0:01:12 15.3M\n",
      " 20 1315M   20  270M    0     0  14.9M      0  0:01:28  0:00:18  0:01:10 15.3M\n",
      " 21 1315M   21  283M    0     0  14.8M      0  0:01:28  0:00:19  0:01:09 15.2M\n",
      " 22 1315M   22  299M    0     0  14.8M      0  0:01:28  0:00:20  0:01:08 15.1M\n",
      " 24 1315M   24  316M    0     0  14.9M      0  0:01:27  0:00:21  0:01:06 15.7M\n",
      " 24 1315M   24  327M    0     0  14.8M      0  0:01:28  0:00:22  0:01:06 15.0M\n",
      " 25 1315M   25  340M    0     0  14.7M      0  0:01:29  0:00:23  0:01:06 14.1M\n",
      " 26 1315M   26  353M    0     0  14.6M      0  0:01:29  0:00:24  0:01:05 14.0M\n",
      " 28 1315M   28  370M    0     0  14.7M      0  0:01:29  0:00:25  0:01:04 14.2M\n",
      " 29 1315M   29  390M    0     0  14.9M      0  0:01:27  0:00:26  0:01:01 14.8M\n",
      " 30 1315M   30  404M    0     0  14.9M      0  0:01:28  0:00:27  0:01:01 15.4M\n",
      " 31 1315M   31  418M    0     0  14.9M      0  0:01:28  0:00:28  0:01:00 15.6M\n",
      " 32 1315M   32  432M    0     0  14.8M      0  0:01:28  0:00:29  0:00:59 15.6M\n",
      " 34 1315M   34  448M    0     0  14.9M      0  0:01:28  0:00:30  0:00:58 15.6M\n",
      " 35 1315M   35  464M    0     0  14.9M      0  0:01:28  0:00:31  0:00:57 14.8M\n",
      " 36 1315M   36  478M    0     0  14.9M      0  0:01:28  0:00:32  0:00:56 14.7M\n",
      " 37 1315M   37  494M    0     0  14.9M      0  0:01:27  0:00:33  0:00:54 15.1M\n",
      " 38 1315M   38  511M    0     0  15.0M      0  0:01:27  0:00:34  0:00:53 15.9M\n",
      " 40 1315M   40  528M    0     0  15.0M      0  0:01:27  0:00:35  0:00:52 15.9M\n",
      " 41 1315M   41  543M    0     0  15.0M      0  0:01:27  0:00:36  0:00:51 15.8M\n",
      " 42 1315M   42  564M    0     0  15.2M      0  0:01:26  0:00:37  0:00:49 17.2M\n",
      " 44 1315M   44  582M    0     0  15.2M      0  0:01:26  0:00:38  0:00:48 17.5M\n",
      " 45 1315M   45  592M    0     0  15.1M      0  0:01:26  0:00:39  0:00:47 16.1M\n",
      " 46 1315M   46  612M    0     0  15.2M      0  0:01:26  0:00:40  0:00:46 16.8M\n",
      " 48 1315M   48  631M    0     0  15.3M      0  0:01:25  0:00:41  0:00:44 17.6M\n",
      " 49 1315M   49  650M    0     0  15.4M      0  0:01:25  0:00:42  0:00:43 17.2M\n",
      " 50 1315M   50  670M    0     0  15.5M      0  0:01:24  0:00:43  0:00:41 17.5M\n",
      " 52 1315M   52  687M    0     0  15.6M      0  0:01:24  0:00:44  0:00:40 19.1M\n",
      " 53 1315M   53  705M    0     0  15.6M      0  0:01:24  0:00:45  0:00:39 18.4M\n",
      " 54 1315M   54  719M    0     0  15.6M      0  0:01:24  0:00:46  0:00:38 17.6M\n",
      " 55 1315M   55  736M    0     0  15.6M      0  0:01:24  0:00:47  0:00:37 17.1M\n",
      " 57 1315M   57  753M    0     0  15.6M      0  0:01:23  0:00:48  0:00:35 16.6M\n",
      " 58 1315M   58  765M    0     0  15.6M      0  0:01:24  0:00:49  0:00:35 15.6M\n",
      " 59 1315M   59  776M    0     0  15.5M      0  0:01:24  0:00:50  0:00:34 14.2M\n",
      " 60 1315M   60  791M    0     0  15.4M      0  0:01:24  0:00:51  0:00:33 14.3M\n",
      " 61 1315M   61  808M    0     0  15.5M      0  0:01:24  0:00:52  0:00:32 14.4M\n",
      " 62 1315M   62  823M    0     0  15.5M      0  0:01:24  0:00:53  0:00:31 14.1M\n",
      " 63 1315M   63  839M    0     0  15.5M      0  0:01:24  0:00:54  0:00:30 14.7M\n",
      " 64 1315M   64  854M    0     0  15.5M      0  0:01:24  0:00:55  0:00:29 15.6M\n",
      " 66 1315M   66  871M    0     0  15.5M      0  0:01:24  0:00:56  0:00:28 16.0M\n",
      " 67 1315M   67  888M    0     0  15.5M      0  0:01:24  0:00:57  0:00:27 15.9M\n",
      " 68 1315M   68  907M    0     0  15.6M      0  0:01:24  0:00:58  0:00:26 16.7M\n",
      " 70 1315M   70  924M    0     0  15.6M      0  0:01:24  0:00:59  0:00:25 17.0M\n",
      " 71 1315M   71  938M    0     0  15.6M      0  0:01:24  0:01:00  0:00:24 16.8M\n",
      " 72 1315M   72  959M    0     0  15.7M      0  0:01:23  0:01:01  0:00:22 17.5M\n",
      " 74 1315M   74  977M    0     0  15.7M      0  0:01:23  0:01:02  0:00:21 17.8M\n",
      " 75 1315M   75  993M    0     0  15.7M      0  0:01:23  0:01:03  0:00:20 17.2M\n",
      " 76 1315M   76 1007M    0     0  15.7M      0  0:01:23  0:01:04  0:00:19 16.5M\n",
      " 78 1315M   78 1028M    0     0  15.8M      0  0:01:23  0:01:05  0:00:18 17.9M\n",
      " 79 1315M   79 1047M    0     0  15.8M      0  0:01:22  0:01:06  0:00:16 17.5M\n",
      " 80 1315M   80 1064M    0     0  15.8M      0  0:01:22  0:01:07  0:00:15 17.4M\n",
      " 82 1315M   82 1086M    0     0  15.9M      0  0:01:22  0:01:08  0:00:14 18.4M\n",
      " 83 1315M   83 1100M    0     0  15.9M      0  0:01:22  0:01:09  0:00:13 18.6M\n",
      " 85 1315M   85 1120M    0     0  15.9M      0  0:01:22  0:01:10  0:00:12 18.3M\n",
      " 86 1315M   86 1139M    0     0  16.0M      0  0:01:22  0:01:11  0:00:11 18.4M\n",
      " 88 1315M   88 1158M    0     0  16.0M      0  0:01:21  0:01:12  0:00:09 18.6M\n",
      " 89 1315M   89 1175M    0     0  16.0M      0  0:01:21  0:01:13  0:00:08 17.8M\n",
      " 90 1315M   90 1193M    0     0  16.1M      0  0:01:21  0:01:14  0:00:07 18.5M\n",
      " 92 1315M   92 1211M    0     0  16.1M      0  0:01:21  0:01:15  0:00:06 18.2M\n",
      " 93 1315M   93 1228M    0     0  16.1M      0  0:01:21  0:01:16  0:00:05 17.7M\n",
      " 94 1315M   94 1245M    0     0  16.1M      0  0:01:21  0:01:17  0:00:04 17.3M\n",
      " 95 1315M   95 1261M    0     0  16.1M      0  0:01:21  0:01:18  0:00:03 17.1M\n",
      " 97 1315M   97 1277M    0     0  16.1M      0  0:01:21  0:01:19  0:00:02 16.6M\n",
      " 98 1315M   98 1296M    0     0  16.1M      0  0:01:21  0:01:20  0:00:01 17.0M\n",
      "100 1315M  100 1315M    0     0  16.2M      0  0:01:20  0:01:20 --:--:-- 18.0M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1352  100  1352    0     0   3436      0 --:--:-- --:--:-- --:--:--  3448\n",
      "\n",
      " 15 71.8M   15 11.3M    0     0  10.3M      0  0:00:06  0:00:01  0:00:05 10.3M\n",
      " 39 71.8M   39 28.2M    0     0  13.4M      0  0:00:05  0:00:02  0:00:03 16.8M\n",
      " 61 71.8M   61 44.3M    0     0  14.3M      0  0:00:05  0:00:03  0:00:02 16.4M\n",
      " 88 71.8M   88 63.7M    0     0  15.5M      0  0:00:04  0:00:04 --:--:-- 17.4M\n",
      "100 71.8M  100 71.8M    0     0  15.5M      0  0:00:04  0:00:04 --:--:-- 17.1M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== train.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"train_1\",\n",
      "        \"question\": \"Who is the author of the book, \\\"Horrors of Slavery, or the American Turf in Tripoli\\\"?\",\n",
      "        \"answer\": \"WILLIAM RAY\",\n",
      "        \"org_answer\": \"WILLIAM RAY\",\n",
      "        \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "        \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, \n",
      "\n",
      "Loaded 439302 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"train_1\",\n",
      "  \"question\": \"Who is the author of the book, \\\"Horrors of Slavery, or the American Turf in Tripoli\\\"?\",\n",
      "  \"answer\": \"WILLIAM RAY\",\n",
      "  \"org_answer\": \"WILLIAM RAY\",\n",
      "  \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "  \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, or the American Turf in Tripoli,\\u201d VOTARIES of Freedom, arm! The British Lion roars! Legions of Valor, take th\\u2019 alarm\\u2014; Rash, ru\n",
      "\n",
      "===== validation.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"val_1\",\n",
      "        \"question\": \"How much of the crew would Gerry want to shore up in a gale of wind?\",\n",
      "        \"answer\": \"half\",\n",
      "        \"org_answer\": \"half\",\n",
      "        \"para_id\": \"Maine_18100326_13\",\n",
      "        \"context\": \"But my lads, it was right to put in federal officers last year, and the crew knew it; they saw what was brewing well enough, and by the soul of me, if the federal men hadn't done what they did, Old Davy Jones would have had his clutches into our quarters \n",
      "\n",
      "Loaded 24111 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"val_1\",\n",
      "  \"question\": \"How much of the crew would Gerry want to shore up in a gale of wind?\",\n",
      "  \"answer\": \"half\",\n",
      "  \"org_answer\": \"half\",\n",
      "  \"para_id\": \"Maine_18100326_13\",\n",
      "  \"context\": \"But my lads, it was right to put in federal officers last year, and the crew knew it; they saw what was brewing well enough, and by the soul of me, if the federal men hadn't done what they did, Old Davy Jones would have had his clutches into our quarters long ago. And now who d'ye think they want to put in instead of our present Commander C. GORE, and the Chief Mate COBB, an old weather-beaten \n",
      "\n",
      "===== test.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"test_1\",\n",
      "        \"question\": \"How many lots did Thomas Peirce have?\",\n",
      "        \"answer\": \"183\",\n",
      "        \"org_answer\": \"183\",\n",
      "        \"para_id\": \"New_Hampshire_18030125_16\",\n",
      "        \"context\": \"Axivil Roberts, part of lot 180 108 60 Capt. George Walker, 181 140 35 George Townson, 183 48 19 Samuel Snell, 184 36 9 Samuel Waterhouse, 185 24 6 John Parker, 186 36 10 John Davis, 187 45 20 John Cross, 188 15 4 Benjamin Cross, 189 50 13 Widow Gilman, 209 21 8 George Peirce, 2\n",
      "\n",
      "Loaded 24084 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"test_1\",\n",
      "  \"question\": \"How many lots did Thomas Peirce have?\",\n",
      "  \"answer\": \"183\",\n",
      "  \"org_answer\": \"183\",\n",
      "  \"para_id\": \"New_Hampshire_18030125_16\",\n",
      "  \"context\": \"Axivil Roberts, part of lot 180 108 60 Capt. George Walker, 181 140 35 George Townson, 183 48 19 Samuel Snell, 184 36 9 Samuel Waterhouse, 185 24 6 John Parker, 186 36 10 John Davis, 187 45 20 John Cross, 188 15 4 Benjamin Cross, 189 50 13 Widow Gilman, 209 21 8 George Peirce, 209 200 75 Thomas Peirce, 220 185 71 SIXTH RANGE: Col. Henry Sherburne, 241 552 150 Nathaniel Roberts, 249 30 8 Jonathan Patridge, 252 60 18 Jo\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\" -o test.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\" -o train.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\" -o validation.json\n",
    "\n",
    "import json\n",
    "\n",
    "files = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n===== {path} =====\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read a few hundred characters to see what kind of JSON it is\n",
    "            head = f.read(500)\n",
    "            print(\"Preview of first 500 characters:\\n\")\n",
    "            print(head[:500])\n",
    "        # Try to load only part of the file\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"\\nLoaded {len(data)} items (list).\")\n",
    "            print(\"Dictionary keys:\", list(data[0].keys()))\n",
    "            print(json.dumps(data[0], indent=2)[:600])\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"\\nTop-level is a dictionary. Keys:\", list(data.keys()))\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    print(f\"Key '{k}' contains a list of {len(v)} items.\")\n",
    "                    if v:\n",
    "                        print(\"First item keys:\", list(v[0].keys()))\n",
    "                        print(json.dumps(v[0], indent=2)[:600])\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"Unexpected top-level type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse {path} as JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Get the Index of the collection \n",
    "inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index location: C:\\Users\\user\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\\Index\\terrier_inverted_index\n",
      "Indexed documents: 131921\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import json\n",
    "import pandas as pd \n",
    "import shutil\n",
    "\n",
    "#path index folder \n",
    "folder_name = \"Index\"\n",
    "index_file_name = \"terrier_inverted_index\"\n",
    "index_path = os.path.abspath(os.path.join(folder_name, index_file_name))\n",
    "\n",
    "#init pyTer\n",
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "#make a data frame from all the documents\n",
    "#build Data frame and clean the row context field \n",
    "def  get_dataFrame():\n",
    "    json_path = 'document_collection.json'\n",
    "    parquet_path = 'documents.parquet'\n",
    "    if os.path.exists(parquet_path):\n",
    "        df_documents = pd.read_parquet(parquet_path)\n",
    "    else:\n",
    "        with open('document_collection.json', 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "        df_documents = pd.DataFrame(raw_data)\n",
    "        df_documents = df_documents.rename(columns={\"para_id\": \"docno\", \"context\": \"text\"})[[\"docno\", \"text\", \"publication_date\"]]\n",
    "        df_documents = df_documents.set_index('docno')\n",
    "        df_documents.to_parquet(parquet_path)\n",
    "    return df_documents\n",
    "    \n",
    "\n",
    "def get_index(df):\n",
    "    #check if the index exsists\n",
    "    properties_file = os.path.join(index_path, \"data.properties\")\n",
    "    if os.path.exists(properties_file):\n",
    "        return pt.IndexFactory.of(index_path)\n",
    "    print(\"Index is not found, creating a new Index\")\n",
    "    if not os.path.exists(index_path):\n",
    "        os.makedirs(index_path, exist_ok=True)\n",
    "   \n",
    "    \n",
    "    # Build the index\n",
    "    # Build the index using the updated IterDictIndexer signature\n",
    "    # Key parameters now are: meta, text_attrs, meta_reverse, pretokenised, fields, threads\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        index_path,\n",
    "        meta={\"docno\": 40},            # store docno as metadata (up to 20 characters)\n",
    "        text_attrs=[\"text\"],           # which field(s) contain the text\n",
    "        meta_reverse=[\"docno\"],        # enable reverse lookup on docno\n",
    "        pretokenised=False,\n",
    "        fields=False,\n",
    "        threads=1,\n",
    "    )\n",
    "    index_ref = indexer.index(df.to_dict(orient=\"records\"))\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "    \n",
    "df_documents = get_dataFrame()\n",
    "index = get_index(df_documents)\n",
    "\n",
    "# Print a simple summary\n",
    "print(\"Index location:\", index_path)\n",
    "print(\"Indexed documents:\", index.getCollectionStatistics().getNumberOfDocuments())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gettting the text of the document by its \"docno\"\n",
    "def get_text_by_docno(docno):\n",
    "    return df_documents.loc[docno]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# stats of the indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrier Collection Statistics\n",
      "--------------------------------\n",
      "Indexed documents:        131921\n",
      "Unique terms (vocabulary): 236646\n",
      "Total tokens:             15575099\n",
      "Average document length:  118.06\n"
     ]
    }
   ],
   "source": [
    "# Retrieve collection statistics\n",
    "stats = index.getCollectionStatistics()\n",
    "\n",
    "print(\"Terrier Collection Statistics\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Indexed documents:        {stats.getNumberOfDocuments()}\")\n",
    "print(f\"Unique terms (vocabulary): {stats.getNumberOfUniqueTerms()}\")\n",
    "print(f\"Total tokens:             {stats.getNumberOfTokens()}\")\n",
    "print(f\"Average document length:  {stats.getAverageDocumentLength():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First stage retriver with BM25 \n",
    "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qid   docid                     docno  rank      score   query\n",
      "0   1  122024      New_York_19190610_27     0  10.233243  Europe\n",
      "1   1   61883         Texas_18771215_13     1   9.759719  Europe\n",
      "2   1    9637      Maryland_18401110_20     2   9.730148  Europe\n",
      "3   1  110750      Nebraska_19020225_26     3   9.715430  Europe\n",
      "4   1   36539  Rhode_Island_18511030_25     4   9.642500  Europe\n",
      "5   1   47808      Illinois_18680907_40     5   9.578485  Europe\n",
      "6   1  100207         Kansas_18970902_5     6   9.570657  Europe\n",
      "7   1   65964     Tennessee_18740915_14     7   9.556417  Europe\n",
      "8   1   29371        Hawaii_18520522_33     8   9.509314  Europe\n",
      "9   1   38017     Tennessee_18590726_13     9   9.499877  Europe\n"
     ]
    }
   ],
   "source": [
    "#testing the engine \n",
    "query = \"Europe\"\n",
    "results = bm25.search(query)\n",
    "\n",
    "#show first 10 resaults\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hist word vectors embedding\n",
    "using hist word embadding to retrive the closest term to the query term for each decade from 1800 to 1920. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "histwords_master\n",
      "histwords_master\n",
      "Working directory set to: C:\\Users\\user\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'embeddings/sgns/1810-w.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrepresentations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msequentialembedding\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SequentialEmbedding\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     fiction_embeddings = \u001b[43mSequentialEmbedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43membeddings/sgns\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1810\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     similar_words  =fiction_embeddings.get_seq_closest(query_word,\u001b[32m1810\u001b[39m, \u001b[32m110\u001b[39m )\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(similar_words)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\\histwords_master\\representations\\sequentialembedding.py:14\u001b[39m, in \u001b[36mSequentialEmbedding.load\u001b[39m\u001b[34m(cls, path, years, **kwargs)\u001b[39m\n\u001b[32m     12\u001b[39m embeds = collections.OrderedDict()\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m year \u001b[38;5;129;01min\u001b[39;00m years:\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m     embeds[year] = \u001b[43mEmbedding\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43myear\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m SequentialEmbedding(embeds)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\\histwords_master\\representations\\embedding.py:36\u001b[39m, in \u001b[36mEmbedding.load\u001b[39m\u001b[34m(cls, path, normalize, add_context, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path, normalize=\u001b[38;5;28;01mTrue\u001b[39;00m, add_context=\u001b[38;5;28;01mFalse\u001b[39;00m, **kwargs):\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     mat = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m-w.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m add_context:\n\u001b[32m     38\u001b[39m         mat += np.load(path + \u001b[33m\"\u001b[39m\u001b[33m-c.npy\u001b[39m\u001b[33m\"\u001b[39m, mmap_mode=\u001b[33m\"\u001b[39m\u001b[33mc\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\pyTer\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:454\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    452\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     fid = stack.enter_context(\u001b[38;5;28mopen\u001b[39m(os.fspath(file), \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    455\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'embeddings/sgns/1810-w.npy'"
     ]
    }
   ],
   "source": [
    "#changing the root directory to hist_words\n",
    "import sys \n",
    "query_word = \"gay\"\n",
    "print(path.split(\"\\\\\")[-1])\n",
    "\n",
    "project_home = os.path.abspath(\"histwords_master\")\n",
    "if project_home not in sys.path:\n",
    "    sys.path.insert(0, project_home)\n",
    "home_dir = path.split(\"\\\\\")[-1]\n",
    "print(home_dir)\n",
    "if( home_dir != \"histwords_master\"): \n",
    "    os.chdir(project_home)\n",
    "print(f\"Working directory set to: {os.getcwd()}\")\n",
    "try:\n",
    "    from representations.sequentialembedding import SequentialEmbedding\n",
    "    \n",
    "    fiction_embeddings = SequentialEmbedding.load(\"embeddings/sgns\", list(range(1810, 2000, 10)))\n",
    "    \n",
    "    similar_words  =fiction_embeddings.get_seq_closest(query_word,1810, 110 )\n",
    "    print(similar_words)\n",
    "finally:\n",
    "    path = os.getcwd()\n",
    "    home_dir = path.split(\"\\\\\")[-1]\n",
    "    if( home_dir == \"histwords_master\"): \n",
    "        os.chdir(\"..\") # changing root directory to the project root folder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "home_dir = path.split(\"\\\\\")[-1]\n",
    "if( home_dir == \"histwords_master\"): \n",
    "    os.chdir(\"..\") # changing root directory to the original    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query with bm 25 foreach word in the resault of the histWord similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "results = bm25.search(query_word)\n",
    "maximum_length = 100\n",
    "for word in similar_words:\n",
    "    temp_bm25_resaults =  bm25.search(word).head(100)\n",
    "    results = pd.concat([results,temp_bm25_resaults] , ignore_index= True).drop_duplicates(subset='docid', keep='first')\n",
    "    results = results.sort_values(by = \"score\", ascending=False)\n",
    "    results = results.head(100)\n",
    "results['rank'] = range(len(results))\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Create the Document Collection\n",
    "\n",
    "To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\n",
    "\n",
    "para_id: is the id of a paragraph of a news paper page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17568,
     "status": "ok",
     "timestamp": 1762962853135,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "nxch4FUUbxRw",
    "outputId": "d86e4179-defd-49d8-8b68-172c577ed825"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "inputs = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "output = \"document_collection.json\"\n",
    "\n",
    "def load_list_or_empty(path):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(f\"Skipping {path} because it is missing or empty\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        print(f\"Skipping {path} because it is not a list at the top level\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Skipping {path} because it is not valid JSON\")\n",
    "        return []\n",
    "\n",
    "def project(recs):\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        out.append({\n",
    "            \"para_id\": r.get(\"para_id\", \"\"),\n",
    "            \"context\": r.get(\"context\", \"\"),\n",
    "            \"raw_ocr\": r.get(\"raw_ocr\", \"\"),\n",
    "            \"publication_date\": r.get(\"publication_date\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_recs = []\n",
    "for p in inputs:\n",
    "    recs = load_list_or_empty(p)\n",
    "    print(f\"Loaded {len(recs)} records from {p}\")\n",
    "    all_recs.extend(project(recs))\n",
    "\n",
    "# deduplicate by para_id keeping the first one seen\n",
    "uniq = {}\n",
    "for rec in all_recs:\n",
    "    pid = rec.get(\"para_id\", \"\")\n",
    "    if pid and pid not in uniq:\n",
    "        uniq[pid] = rec\n",
    "\n",
    "result = list(uniq.values())\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(result)} records to {output}\")\n",
    "print(json.dumps(result[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-9wljtri-XX"
   },
   "source": [
    "## You should check that the collection you have matches that of the paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snY9dkltgMts"
   },
   "source": [
    "# Create the Test Queries Data Structure\n",
    "\n",
    "We keep the first 10.000 queries due to memory errors in the free colab version.\n",
    "\n",
    "To be comparable, please keep the top 10.000 queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1762962872929,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "7ZOmr1qBgRxi",
    "outputId": "1a4cbaaa-2813-4814-e0de-aee5aab98f7c"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "input_file = \"test.json\"\n",
    "output_file = \"test_queries.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"question\": clean_question(item.get(\"question\", \"\")),\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Sort by query_id (assuming numeric)\n",
    "queries = sorted(queries, key=lambda x: int(x[\"query_id\"]) if str(x[\"query_id\"]).isdigit() else x[\"query_id\"])\n",
    "\n",
    "# Keep only the first 10,000\n",
    "queries = queries[:10000]\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(queries)} entries to {output_file}\")\n",
    "print(json.dumps(queries[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyCV6oqjFS0"
   },
   "source": [
    "# Create the Qrels for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1762962873672,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "Lxms9bHpjIcn",
    "outputId": "26e9db71-b590-4f5f-94db-484d857db80c"
   },
   "outputs": [],
   "source": [
    "input_file = \"test.json\"\n",
    "qrels_file = \"test_qrels.json\"\n",
    "answers_file = \"test_query_answers.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the qrels file: query_id, iteration=0, para_id, relevance=1\n",
    "qrels = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Build the query_answers file: same plus answer and org_answer\n",
    "query_answers = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1,\n",
    "        \"answer\": item.get(\"answer\", \"\"),\n",
    "        \"org_answer\": item.get(\"org_answer\", \"\")\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Save both files\n",
    "with open(qrels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(answers_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_answers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qrels)} entries to {qrels_file}\")\n",
    "print(f\"Saved {len(query_answers)} entries to {answers_file}\")\n",
    "print(\"Sample qrels entry:\", qrels[0])\n",
    "print(\"Sample query_answers entry:\", query_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7vkoP010nIF"
   },
   "source": [
    "# Retrieval - Good Luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yEes2xbJ0o4S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
