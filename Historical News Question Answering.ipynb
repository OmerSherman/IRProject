{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# Download and Inspect the Collection\n",
    "\n",
    "The dataset was created from the Chronicling America collection — over 21 million digitized newspaper pages (1756–1963) curated by the Library of Congress and NEH. They used 39,330 pages (1800–1920), representing 53 US states, to ensure wide geographic and temporal coverage.\n",
    "\n",
    "Source: https://dl.acm.org/doi/pdf/10.1145/3626772.3657891\n",
    "\n",
    "GitHub: https://github.com/DataScienceUIBK/ChroniclingAmericaQA?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12366,
     "status": "ok",
     "timestamp": 1762962835550,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "4xBdfDsPYdLA",
    "outputId": "32103be7-1880-4ccb-ea70-6800e841dec1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1352  100  1352    0     0   4471      0 --:--:-- --:--:-- --:--:--  4521\n",
      "\n",
      " 16 71.5M   16 11.6M    0     0  12.6M      0  0:00:05 --:--:--  0:00:05 12.6M\n",
      " 60 71.5M   60 43.1M    0     0  22.5M      0  0:00:03  0:00:01  0:00:02 31.5M\n",
      " 89 71.5M   89 63.8M    0     0  21.8M      0  0:00:03  0:00:02  0:00:01 26.1M\n",
      "100 71.5M  100 71.5M    0     0  23.0M      0  0:00:03  0:00:03 --:--:-- 27.3M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1356  100  1356    0     0   6352      0 --:--:-- --:--:-- --:--:--  6457\n",
      "\n",
      "  0 1315M    0 12.1M    0     0  17.1M      0  0:01:16 --:--:--  0:01:16 17.1M\n",
      "  3 1315M    3 45.2M    0     0  26.5M      0  0:00:49  0:00:01  0:00:48 33.1M\n",
      "  4 1315M    4 64.6M    0     0  23.8M      0  0:00:55  0:00:02  0:00:53 26.2M\n",
      "  7 1315M    7 92.5M    0     0  24.9M      0  0:00:52  0:00:03  0:00:49 26.7M\n",
      "  9 1315M    9  130M    0     0  27.5M      0  0:00:47  0:00:04  0:00:43 29.3M\n",
      " 12 1315M   12  167M    0     0  29.3M      0  0:00:44  0:00:05  0:00:39 31.0M\n",
      " 14 1315M   14  193M    0     0  28.8M      0  0:00:45  0:00:06  0:00:39 29.6M\n",
      " 17 1315M   17  229M    0     0  29.8M      0  0:00:44  0:00:07  0:00:37 33.0M\n",
      " 20 1315M   20  270M    0     0  31.0M      0  0:00:42  0:00:08  0:00:34 35.5M\n",
      " 22 1315M   22  292M    0     0  30.1M      0  0:00:43  0:00:09  0:00:34 32.6M\n",
      " 23 1315M   23  306M    0     0  28.6M      0  0:00:45  0:00:10  0:00:35 27.9M\n",
      " 26 1315M   26  347M    0     0  29.6M      0  0:00:44  0:00:11  0:00:33 30.7M\n",
      " 29 1315M   29  389M    0     0  30.6M      0  0:00:42  0:00:12  0:00:30 31.9M\n",
      " 32 1315M   32  433M    0     0  31.5M      0  0:00:41  0:00:13  0:00:28 32.5M\n",
      " 34 1315M   34  457M    0     0  30.9M      0  0:00:42  0:00:14  0:00:28 32.6M\n",
      " 35 1315M   35  463M    0     0  29.5M      0  0:00:44  0:00:15  0:00:29 31.3M\n",
      " 38 1315M   38  508M    0     0  30.4M      0  0:00:43  0:00:16  0:00:27 32.2M\n",
      " 42 1315M   42  564M    0     0  31.8M      0  0:00:41  0:00:17  0:00:24 35.0M\n",
      " 45 1315M   45  604M    0     0  32.3M      0  0:00:40  0:00:18  0:00:22 34.2M\n",
      " 49 1315M   49  652M    0     0  33.1M      0  0:00:39  0:00:19  0:00:20 39.5M\n",
      " 53 1315M   53  702M    0     0  33.9M      0  0:00:38  0:00:20  0:00:18 47.6M\n",
      " 55 1315M   55  734M    0     0  33.8M      0  0:00:38  0:00:21  0:00:17 45.0M\n",
      " 59 1315M   59  780M    0     0  34.3M      0  0:00:38  0:00:22  0:00:16 43.2M\n",
      " 62 1315M   62  828M    0     0  34.9M      0  0:00:37  0:00:23  0:00:14 44.8M\n",
      " 66 1315M   66  876M    0     0  35.4M      0  0:00:37  0:00:24  0:00:13 44.7M\n",
      " 69 1315M   69  917M    0     0  35.6M      0  0:00:36  0:00:25  0:00:11 43.0M\n",
      " 72 1315M   72  954M    0     0  35.7M      0  0:00:36  0:00:26  0:00:10 44.0M\n",
      " 75 1315M   75  998M    0     0  36.0M      0  0:00:36  0:00:27  0:00:09 43.4M\n",
      " 79 1315M   79 1043M    0     0  36.3M      0  0:00:36  0:00:28  0:00:08 42.9M\n",
      " 83 1315M   83 1091M    0     0  36.7M      0  0:00:35  0:00:29  0:00:06 43.1M\n",
      " 86 1315M   86 1139M    0     0  37.0M      0  0:00:35  0:00:30  0:00:05 44.2M\n",
      " 89 1315M   89 1179M    0     0  37.1M      0  0:00:35  0:00:31  0:00:04 44.9M\n",
      " 93 1315M   93 1223M    0     0  37.4M      0  0:00:35  0:00:32  0:00:03 45.1M\n",
      " 96 1315M   96 1266M    0     0  37.5M      0  0:00:35  0:00:33  0:00:02 44.6M\n",
      " 99 1315M   99 1303M    0     0  37.5M      0  0:00:35  0:00:34  0:00:01 42.2M\n",
      "100 1315M  100 1315M    0     0  37.5M      0  0:00:34  0:00:34 --:--:-- 41.1M\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100  1346  100  1346    0     0   6079      0 --:--:-- --:--:-- --:--:--  6174\n",
      "\n",
      " 10 71.8M   10 7872k    0     0  12.7M      0  0:00:05 --:--:--  0:00:05 12.7M\n",
      " 61 71.8M   61 44.4M    0     0  27.7M      0  0:00:02  0:00:01  0:00:01 36.7M\n",
      "100 71.8M  100 71.8M    0     0  30.4M      0  0:00:02  0:00:02 --:--:-- 36.5M\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== train.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"train_1\",\n",
      "        \"question\": \"Who is the author of the book, \\\"Horrors of Slavery, or the American Turf in Tripoli\\\"?\",\n",
      "        \"answer\": \"WILLIAM RAY\",\n",
      "        \"org_answer\": \"WILLIAM RAY\",\n",
      "        \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "        \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, \n",
      "\n",
      "Loaded 439302 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"train_1\",\n",
      "  \"question\": \"Who is the author of the book, \\\"Horrors of Slavery, or the American Turf in Tripoli\\\"?\",\n",
      "  \"answer\": \"WILLIAM RAY\",\n",
      "  \"org_answer\": \"WILLIAM RAY\",\n",
      "  \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "  \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, or the American Turf in Tripoli,\\u201d VOTARIES of Freedom, arm! The British Lion roars! Legions of Valor, take th\\u2019 alarm\\u2014; Rash, ru\n",
      "\n",
      "===== validation.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"val_1\",\n",
      "        \"question\": \"How much of the crew would Gerry want to shore up in a gale of wind?\",\n",
      "        \"answer\": \"half\",\n",
      "        \"org_answer\": \"half\",\n",
      "        \"para_id\": \"Maine_18100326_13\",\n",
      "        \"context\": \"But my lads, it was right to put in federal officers last year, and the crew knew it; they saw what was brewing well enough, and by the soul of me, if the federal men hadn't done what they did, Old Davy Jones would have had his clutches into our quarters \n",
      "\n",
      "Loaded 24111 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"val_1\",\n",
      "  \"question\": \"How much of the crew would Gerry want to shore up in a gale of wind?\",\n",
      "  \"answer\": \"half\",\n",
      "  \"org_answer\": \"half\",\n",
      "  \"para_id\": \"Maine_18100326_13\",\n",
      "  \"context\": \"But my lads, it was right to put in federal officers last year, and the crew knew it; they saw what was brewing well enough, and by the soul of me, if the federal men hadn't done what they did, Old Davy Jones would have had his clutches into our quarters long ago. And now who d'ye think they want to put in instead of our present Commander C. GORE, and the Chief Mate COBB, an old weather-beaten \n",
      "\n",
      "===== test.json =====\n",
      "Preview of first 500 characters:\n",
      "\n",
      "[\n",
      "    {\n",
      "        \"query_id\": \"test_1\",\n",
      "        \"question\": \"How many lots did Thomas Peirce have?\",\n",
      "        \"answer\": \"183\",\n",
      "        \"org_answer\": \"183\",\n",
      "        \"para_id\": \"New_Hampshire_18030125_16\",\n",
      "        \"context\": \"Axivil Roberts, part of lot 180 108 60 Capt. George Walker, 181 140 35 George Townson, 183 48 19 Samuel Snell, 184 36 9 Samuel Waterhouse, 185 24 6 John Parker, 186 36 10 John Davis, 187 45 20 John Cross, 188 15 4 Benjamin Cross, 189 50 13 Widow Gilman, 209 21 8 George Peirce, 2\n",
      "\n",
      "Loaded 24084 items (list).\n",
      "Dictionary keys: ['query_id', 'question', 'answer', 'org_answer', 'para_id', 'context', 'raw_ocr', 'publication_date', 'trans_que', 'trans_ans', 'url']\n",
      "{\n",
      "  \"query_id\": \"test_1\",\n",
      "  \"question\": \"How many lots did Thomas Peirce have?\",\n",
      "  \"answer\": \"183\",\n",
      "  \"org_answer\": \"183\",\n",
      "  \"para_id\": \"New_Hampshire_18030125_16\",\n",
      "  \"context\": \"Axivil Roberts, part of lot 180 108 60 Capt. George Walker, 181 140 35 George Townson, 183 48 19 Samuel Snell, 184 36 9 Samuel Waterhouse, 185 24 6 John Parker, 186 36 10 John Davis, 187 45 20 John Cross, 188 15 4 Benjamin Cross, 189 50 13 Widow Gilman, 209 21 8 George Peirce, 209 200 75 Thomas Peirce, 220 185 71 SIXTH RANGE: Col. Henry Sherburne, 241 552 150 Nathaniel Roberts, 249 30 8 Jonathan Patridge, 252 60 18 Jo\n"
     ]
    }
   ],
   "source": [
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/test.json?download=true\" -o test.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/train.json?download=true\" -o train.json\n",
    "!curl -L \"https://huggingface.co/datasets/Bhawna/ChroniclingAmericaQA/resolve/main/dev.json?download=true\" -o validation.json\n",
    "\n",
    "import json\n",
    "\n",
    "files = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "\n",
    "for path in files:\n",
    "    print(f\"\\n===== {path} =====\")\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            # Read a few hundred characters to see what kind of JSON it is\n",
    "            head = f.read(500)\n",
    "            print(\"Preview of first 500 characters:\\n\")\n",
    "            print(head[:500])\n",
    "        # Try to load only part of the file\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            print(f\"\\nLoaded {len(data)} items (list).\")\n",
    "            print(\"Dictionary keys:\", list(data[0].keys()))\n",
    "            print(json.dumps(data[0], indent=2)[:600])\n",
    "        elif isinstance(data, dict):\n",
    "            print(\"\\nTop-level is a dictionary. Keys:\", list(data.keys()))\n",
    "            for k, v in data.items():\n",
    "                if isinstance(v, list):\n",
    "                    print(f\"Key '{k}' contains a list of {len(v)} items.\")\n",
    "                    if v:\n",
    "                        print(\"First item keys:\", list(v[0].keys()))\n",
    "                        print(json.dumps(v[0], indent=2)[:600])\n",
    "                        break\n",
    "        else:\n",
    "            print(f\"Unexpected top-level type: {type(data)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not parse {path} as JSON: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Create the Document Collection\n",
    "\n",
    "To do that, we create a new json file that contains the 'para_id', 'context', 'raw_ocr', 'publication_date' keys, for all para_id in the collection.\n",
    "\n",
    "para_id: is the id of a paragraph of a news paper page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17568,
     "status": "ok",
     "timestamp": 1762962853135,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "nxch4FUUbxRw",
    "outputId": "d86e4179-defd-49d8-8b68-172c577ed825"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 439302 records from train.json\n",
      "Loaded 24111 records from validation.json\n",
      "Loaded 24084 records from test.json\n",
      "Wrote 131921 records to document_collection.json\n",
      "[\n",
      "  {\n",
      "    \"para_id\": \"New_Hampshire_18070804_1\",\n",
      "    \"context\": \"Aiscellaneous Repository. From the Albany Register, WAR, OR A PROSPECT OF IT, From recent instances of British Outrage. BY: WILLIAM RAY, Author of the contemplated publication, entitled, \\u201cHorrors of Slavery, or the American Turf in Tripoli,\\u201d VOTARIES of Freedom, arm! The British Lion roars! Legions of Valor, take th\\u2019 alarm\\u2014; Rash, rush to guard our shores! Behold the horrid deed\\u2014 Your brethren gasping lie! Beneath a tyrant\\u2019s hand they bleed\\u2014 They groan\\u2014they faint\\u2014they die. Veterans of seventy-six, Awake the slumbering sword;\\u2014 Hearts of your murderous foes transfix\\u2014 'Tis vengeance gives the word. Remember Lexington, And Bunker\\u2019s tragic hill; \\u201cThe same who spilt your blood thereon, Your blood again would spill. Ye who have seen your wives, Your children, and your fires, Too British ruffians yield their lives, And roast in savage fires;\\u2014 Our cities lost in flames,\\u2014 Your mothers captive led; Rise and avenge their injured names, Ye kindred of the dead. But not Revenge alone, Should urge you to the field; Let Duty lead you firmly on, And justice be your shield. Sure as we fail to join And crush our impious foes, War, fire and sword, and death combine, And woes succeed to woes.\",\n",
      "    \"raw_ocr\": \"fAiscellancous Bepogitory.\\n. dvom the Albany Regifier,\\n. . WAR,OR A PROSPECT OF IT,\\nFrom recent inflances of Britifp Oulrage.\\n BY: WILLIAM RAY:\\nHAuthsr of the tontemplated publication, entitled,\\n\\u00ab Horrors of Slavery,or the American Turg\\nin Tripoli,\\u201d\\nVOT\\u2019RIES of Freedom, arm!\\n The British Lion roars !\\n Legions of Valor, take th\\u2019 alarm\\u2014 ;\\nRash, rush to guard our shores ! -\\n- Behold the horrid deed\\u2014 v\\n. Your brethren gasping lie!\\n Beneath a tyrant\\u2019s hand they bleed\\u2014 -\\nThey groan--they faint\\u2014they die.\\n _Vet\\u2019rans of seventy-six, i\\n. Awake the flumb\\u2019ring sword ;\\u2014 = *.\\n- Hearts of your murd\\u2019rous foes transfix\\u2014\\n*Tis vengeance gives the word.\\n Remember Lexington,\\nAnd Bunker\\u2019s tragic hill;\\n\\u201cThe fame who {pilt your blnod thereon,\\n- Your blnod again would spill,\\nYe who have fe\\u00e9n your wives,\\nYour childrea, and your fires,\\n7\\u2019oo British ruffians\\u2019 yield their lives, .\\n Apd roast in savage fires;\\u2014\\nQer cities loft in lames,\\u2014\\nYour mothers captive led;\\nRife and avenge their injur'd names, -\\nYe kindred of the dead: .\\n But not Revenge alone,\\nShould \\u2018grge you to the field ;\\n. Let Duty lead you firmly on, :\\n. Aond jultice be your shield.\\n Sure as we fail to join\\n\\u00a2 And eruih our impeous foes,\\n\\u00bb Woar, fire and sword, and dcath combie,\\n- And woes succeed to woes.\",\n",
      "    \"publication_date\": \"1807-08-04\"\n",
      "  },\n",
      "  {\n",
      "    \"para_id\": \"New_Hampshire_18070804_4\",\n",
      "    \"context\": \"Surely he above the rest of his fellow mortals, partakes of heaven here below, of bliss which none but the virtuous ever claim. \\u00a5 Obituary B In France, Gen. de Rosemberg, aged 83, formerly Marshal of France, Grand Officer of the Legion of Honor, and commander of the French troops in the United States during the Revolutionary war. \\u00a5 In Washington, Hon. Uriah Tracy, Esq. Senator of the United States from the State of Connecticut, aged 54\\u2014his pall was supported by the heads of department and officers of government\\u2014he had been sick at Washington since March last. In Baltimore, during the week ending the 18th ult. 15 adults and 23 children. In Philadelphia, during the week ending 18th ult.\\u201424 adults and 40 children. In New-York, during the week ending 18th ult, 2 men, 6 women, 10 boys and 7 girls. At Newark, (N. J.) the Rev. Dr. Alexander Macauley, aged 73. At Danvers, Dr. Amos Putnam, aged 83. At Andover, Mrs. Susanna Symmes, relict of the late Rev. Dr. Wm. Symmes, aged 79. At Salem, (Mass) Widow Margaret Swaffey, aged 100 years and 6 months. At Newburyport, Mr. Samuel Dexter, aged 36, only son of the late Lord T. Dexter. At Brunswick, Rev. Joseph McKean, D. D, late president of Bowdoin college. At Lancaster, Mr. Henry Haskell, aged 73, a Lt. Col. in the revolutionary army. At Chesterfield, Mrs. Louisa Parsons, wife of Benjamin Parsons, Esq. aged 39.\",\n",
      "    \"raw_ocr\": \"Surely he a\\nbove the rest of his fellow mortals, partakes\\nof heaven hcre below, of bliss which nene\\nbut the virtuous ever claim.\\n \\u00a5 OvoioDlEDoitry B\\n In France, Gen.de Rosbambesu, aged 8,\\nfarmerly s Marthall of France, Grand Ofa\\ncer of the Legion of Henor,and commander\\nof the Freach troops in the United States\\nduring the Revolutienary war. \\u00a5\\n In Wathington, Hon. Uriab Tracey, Esq.\\nYenator of the United States from-the State\\nof Connecticut, aged s4~\\u2014his pall was sup\\nported by the, heads of department and\\nofficers of goverament\\u2014he had been sick at\\nWashington since March lag.\\n In Baltumore, during the week ending the\\n18th uvit. 15 adults and 23 childiren.\\n In Philadelphia, during the week ending\\n18th u1t.~\\u201424 adults and 40 children.\\n ~ln New-York, during the week ending\\n18th nlt, 2 men, 6 womcen, 10 boys and 7\\ngirls, . : ' \\u00a2 \\u00a5 oo\\nAt Newark, (N. ].) the Rev, Dr. Alexander\\nMacauwgrther, aged 73. ; (\\n. At Daovers, Dt. Amos Putnam,aged 83.\\n At Andover, Mrs. Susanna Symmes, relict\\nof the late Rev. Dr. Wm. Symmes, aged 79.\\n At Salem, (Mass) Widow Margaret Swafey,\\naged 100 yearsand 6 months.\\n. At Newburyport, Mr.\\\" Samuel Dexter, aged\\n- 36, only son of the late Lord T. Dexter.\\n o At Bruafwick, Rev. Fo/eph McKean, D. D,\\nJate president of Bowdoin college.\\n At Lancaster, Mr. Henry Hajkell, aged 73,\\na Lt, Col. in the revolutionary army.\\n At Cheflterfield, Mrs Louisa Parsons, wifoy\\n-of Benjamin Parlons, Efq.aged 39. i\\n-\",\n",
      "    \"publication_date\": \"1807-08-04\"\n",
      "  },\n",
      "  {\n",
      "    \"para_id\": \"New_Hampshire_18070804_5\",\n",
      "    \"context\": \"At Westmoreland, Mrs. Sally Lincoln, wife of Mr. Spencer L. aged 28.  At Henrico, Mrs. Polly Adams, consort On Saturday, the 11th ult. Mr. Joseph Meyer, of Hampstead, was found dead in the road, (his horse standing by him) when oy e g e SMITH & RUST Pocket Book Lost.  \\\"LOST last Wednesday between 7 and 8 o\\u2019clock in the afternoon, either in the Globe Tavern at the Plains, or on the road leading from thence to Portsmouth, a new Red Morocco Pocket Book ; containing some Money, Notes of hand payable to the Subscriber, also, New Hampshire Fire and Marine Certificates, and other papers valuable to none but to the owner\\u2014Whoever shall find said Pocket Book, and re- turn it with its contents, with or without the money shall be handsomely rewarded, and the thanks of their humble servant EDWARD. PARRY.  TO BE LET, That Fireproof Store lately improved by Mr. Benjamin Swett, which must be allowed to be the best stand for business, either for English or West- India Goods in this town\\u2014Inquire of EDWARD PARRY, Who has a large assortment of the fashionable GOODS, for sale cheap for cash or short credit. July 28.  FOR SALE, A NEW GONDOLA, built of the best materials, and by an experienced workman, forty feet cor- ner piece,\\u2014For further particulars enquire of MICHAEL WIGGIN. Newmarket, July 27th, 1807 2. CHAISE.\",\n",
      "    \"raw_ocr\": \"At Weltmoreland, Mrs. Sally Liacoln, wife\\n~of Mr,Spencer L.aged 28. 77\\niAt Hennikee, Mrs. Polly Adams, consort\\n00 Baturday, the lith vit. Mr. Jo/iph\\nNeyer, of Hampftead, was found dead in the\\nroud. (his Borfe Randing by him) sbens oy\\ne g e\\nSMR R Y e sT R\\nPocket Book Loft.\\n i\\u201c OSS i'laft Wednesday between 7-\\nJM_4 and 8 o\\u2019clock in the afternoon,\\neither in the Globe Tavern at the\\nPlains, or cn the road leading from\\nthence to Portsmouth, a new j\\nRed Morscco Pocket Book ;\\ncontaining some Money, Notes of hand\\npayable to the Subscriber. alfs,\\nNewhampfhire Fire and Marine\\nCertificates, and other papers valuable\\nto no ore but to the owner\\u2014Whoev\\ner shall find said Pocket Book, and re-,\\nturn it wich its contents, with or with.\\n out the money {hall be hand{omely re\\nwarded, aand the thanks of their hum.\\nble servant EDWARD. PARRY.\\n 10 BE LET,\\nThat Fire proof Store lately improv\\ned by Mr. Benjamin Swett, which\\nmust be allowed to be the best stand\\nfor business, either for English or Welt-\\nIndia Goods in this town\\u2014llinquire\\nof \\\" EDWARD PARRY,\\nWho has a large assortment fieth\\nfathionable GOQODS, for {ale cheap\\nfor cath or short credit. Fuly 28.\\n FOR SALE,\\nANEW GUNDALO,\\nbuilt of the belt materials, and by an\\nexperienced workman, forty feet cor\\nner piece,\\u2014For further particulars\\nenquire of\\n\\\" MICHAEL WIGGIN.\\nNewmarket, July 27th, 1807\\n2 .\\nCHAISE.\\n\",\n",
      "    \"publication_date\": \"1807-08-04\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "inputs = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "output = \"document_collection.json\"\n",
    "\n",
    "def load_list_or_empty(path):\n",
    "    if not os.path.exists(path) or os.path.getsize(path) == 0:\n",
    "        print(f\"Skipping {path} because it is missing or empty\")\n",
    "        return []\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            return data\n",
    "        print(f\"Skipping {path} because it is not a list at the top level\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Skipping {path} because it is not valid JSON\")\n",
    "        return []\n",
    "\n",
    "def project(recs):\n",
    "    out = []\n",
    "    for r in recs:\n",
    "        out.append({\n",
    "            \"para_id\": r.get(\"para_id\", \"\"),\n",
    "            \"context\": r.get(\"context\", \"\"),\n",
    "            \"raw_ocr\": r.get(\"raw_ocr\", \"\"),\n",
    "            \"publication_date\": r.get(\"publication_date\", \"\")\n",
    "        })\n",
    "    return out\n",
    "\n",
    "all_recs = []\n",
    "for p in inputs:\n",
    "    recs = load_list_or_empty(p)\n",
    "    print(f\"Loaded {len(recs)} records from {p}\")\n",
    "    all_recs.extend(project(recs))\n",
    "\n",
    "# deduplicate by para_id keeping the first one seen\n",
    "uniq = {}\n",
    "for rec in all_recs:\n",
    "    pid = rec.get(\"para_id\", \"\")\n",
    "    if pid and pid not in uniq:\n",
    "        uniq[pid] = rec\n",
    "\n",
    "result = list(uniq.values())\n",
    "\n",
    "with open(output, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(result, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Wrote {len(result)} records to {output}\")\n",
    "print(json.dumps(result[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-9wljtri-XX"
   },
   "source": [
    "## You should check that the collection you have matches that of the paper!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snY9dkltgMts"
   },
   "source": [
    "# Create the Test Queries Data Structure\n",
    "\n",
    "We keep the first 10.000 queries due to memory errors in the free colab version.\n",
    "\n",
    "To be comparable, please keep the top 10.000 queries for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1151,
     "status": "ok",
     "timestamp": 1762962872929,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "7ZOmr1qBgRxi",
    "outputId": "1a4cbaaa-2813-4814-e0de-aee5aab98f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 entries to test_queries.json\n",
      "[\n",
      "  {\n",
      "    \"query_id\": \"test_1\",\n",
      "    \"question\": \"How many lots did Thomas Peirce have\"\n",
      "  },\n",
      "  {\n",
      "    \"query_id\": \"test_10\",\n",
      "    \"question\": \"Who gave Hamilton the substance of what he had proposed on the part of General Hamilton\"\n",
      "  },\n",
      "  {\n",
      "    \"query_id\": \"test_100\",\n",
      "    \"question\": \"Who informs his FRIENDS and the PUBLIC that he has taken that justly celebrated INN in this city\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "input_file = \"test.json\"\n",
    "output_file = \"test_queries.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "def clean_question(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize(\"NFKC\", text)\n",
    "    text = re.sub(rf\"[{re.escape(string.punctuation)}]\", \" \", text)  # remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # collapse multiple spaces\n",
    "    return text.strip()\n",
    "\n",
    "# Extract and clean\n",
    "queries = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"question\": clean_question(item.get(\"question\", \"\")),\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Sort by query_id (assuming numeric)\n",
    "queries = sorted(queries, key=lambda x: int(x[\"query_id\"]) if str(x[\"query_id\"]).isdigit() else x[\"query_id\"])\n",
    "\n",
    "# Keep only the first 10,000\n",
    "queries = queries[:10000]\n",
    "\n",
    "# Save new JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(queries, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(queries)} entries to {output_file}\")\n",
    "print(json.dumps(queries[:3], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NyCV6oqjFS0"
   },
   "source": [
    "# Create the Qrels for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 742,
     "status": "ok",
     "timestamp": 1762962873672,
     "user": {
      "displayName": "Georgios Peikos",
      "userId": "04834132442165285194"
     },
     "user_tz": -60
    },
    "id": "Lxms9bHpjIcn",
    "outputId": "26e9db71-b590-4f5f-94db-484d857db80c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 24084 entries to test_qrels.json\n",
      "Saved 24084 entries to test_query_answers.json\n",
      "Sample qrels entry: {'query_id': 'test_1', 'iteration': 0, 'para_id': 'New_Hampshire_18030125_16', 'relevance': 1}\n",
      "Sample query_answers entry: {'query_id': 'test_1', 'iteration': 0, 'para_id': 'New_Hampshire_18030125_16', 'relevance': 1, 'answer': '183', 'org_answer': '183'}\n"
     ]
    }
   ],
   "source": [
    "input_file = \"test.json\"\n",
    "qrels_file = \"test_qrels.json\"\n",
    "answers_file = \"test_query_answers.json\"\n",
    "\n",
    "# Load the data\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Build the qrels file: query_id, iteration=0, para_id, relevance=1\n",
    "qrels = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Build the query_answers file: same plus answer and org_answer\n",
    "query_answers = [\n",
    "    {\n",
    "        \"query_id\": item.get(\"query_id\", \"\"),\n",
    "        \"iteration\": 0,\n",
    "        \"para_id\": item.get(\"para_id\", \"\"),\n",
    "        \"relevance\": 1,\n",
    "        \"answer\": item.get(\"answer\", \"\"),\n",
    "        \"org_answer\": item.get(\"org_answer\", \"\")\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Save both files\n",
    "with open(qrels_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(answers_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(query_answers, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(qrels)} entries to {qrels_file}\")\n",
    "print(f\"Saved {len(query_answers)} entries to {answers_file}\")\n",
    "print(\"Sample qrels entry:\", qrels[0])\n",
    "print(\"Sample query_answers entry:\", query_answers[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7vkoP010nIF"
   },
   "source": [
    "# Retrieval - Good Luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mylmVIP9bu8y"
   },
   "source": [
    "# Get the Index of the collection and store the document as a dataframe\n",
    "inverted index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index location: C:\\Users\\user\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\\Index\\terrier_inverted_index\n",
      "Indexed documents: 131921\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyterrier as pt\n",
    "import json\n",
    "import pandas as pd \n",
    "import shutil\n",
    "\n",
    "#path index folder \n",
    "folder_name = \"Index\"\n",
    "index_file_name = \"terrier_inverted_index\"\n",
    "index_path = os.path.abspath(os.path.join(folder_name, index_file_name))\n",
    "\n",
    "#init pyTer\n",
    "if not pt.java.started():\n",
    "    pt.java.init()\n",
    "\n",
    "#make a data frame from all the documents\n",
    "#build Data frame and clean the row context field \n",
    "def  get_dataFrame():\n",
    "    json_path = 'document_collection.json'\n",
    "    parquet_path = 'documents.parquet'\n",
    "    if os.path.exists(parquet_path):\n",
    "        df_documents = pd.read_parquet(parquet_path)\n",
    "    else:\n",
    "        with open('document_collection.json', 'r', encoding='utf-8') as f:\n",
    "            raw_data = json.load(f)\n",
    "        df_documents = pd.DataFrame(raw_data)\n",
    "        df_documents = df_documents.rename(columns={\"para_id\": \"docno\", \"context\": \"text\"})[[\"docno\", \"text\", \"publication_date\"]]\n",
    "        df_documents = df_documents.set_index('docno')\n",
    "        df_documents.to_parquet(parquet_path)\n",
    "    return df_documents\n",
    "    \n",
    "\n",
    "def get_index(df):\n",
    "    #check if the index exsists\n",
    "    properties_file = os.path.join(index_path, \"data.properties\")\n",
    "    if os.path.exists(properties_file):\n",
    "        return pt.IndexFactory.of(index_path)\n",
    "    print(\"Index is not found, creating a new Index\")\n",
    "    if not os.path.exists(index_path):\n",
    "        os.makedirs(index_path, exist_ok=True)\n",
    "   \n",
    "    \n",
    "    # Build the index\n",
    "    # Build the index using the updated IterDictIndexer signature\n",
    "    # Key parameters now are: meta, text_attrs, meta_reverse, pretokenised, fields, threads\n",
    "    indexer = pt.IterDictIndexer(\n",
    "        index_path,\n",
    "        meta={\"docno\": 40},            # store docno as metadata (up to 20 characters)\n",
    "        text_attrs=[\"text\"],           # which field(s) contain the text\n",
    "        meta_reverse=[\"docno\"],        # enable reverse lookup on docno\n",
    "        pretokenised=False,\n",
    "        fields=False,\n",
    "        threads=1,\n",
    "    )\n",
    "    index_ref = indexer.index(df.to_dict(orient=\"records\"))\n",
    "    return pt.IndexFactory.of(index_ref)\n",
    "    \n",
    "df_documents = get_dataFrame()\n",
    "index = get_index(df_documents)\n",
    "\n",
    "# Print a simple summary\n",
    "print(\"Index location:\", index_path)\n",
    "print(\"Indexed documents:\", index.getCollectionStatistics().getNumberOfDocuments())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBwc_amYdiZ"
   },
   "source": [
    "# stats of the indexing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrier Collection Statistics\n",
      "--------------------------------\n",
      "Indexed documents:        131921\n",
      "Unique terms (vocabulary): 236646\n",
      "Total tokens:             15575099\n",
      "Average document length:  118.06\n"
     ]
    }
   ],
   "source": [
    "# Retrieve collection statistics\n",
    "stats = index.getCollectionStatistics()\n",
    "\n",
    "print(\"Terrier Collection Statistics\")\n",
    "print(\"--------------------------------\")\n",
    "print(f\"Indexed documents:        {stats.getNumberOfDocuments()}\")\n",
    "print(f\"Unique terms (vocabulary): {stats.getNumberOfUniqueTerms()}\")\n",
    "print(f\"Total tokens:             {stats.getNumberOfTokens()}\")\n",
    "print(f\"Average document length:  {stats.getAverageDocumentLength():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# querry processing with NER to obtain relevant terms\n",
    "We use SpaCy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "#download SpaCy\n",
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import math\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n",
    "\n",
    "def sortByIDF(tokens_list, index ): #sort a list by the IDF score of the tockens, return a list of tupples by this format: (token , idf score of tokens)\n",
    "    tokens_with_scores = [(token, get_word_importance(token, index)) for token in tokens_list  ]\n",
    "    tokens_with_scores.sort(reverse = True, key = lambda tupple: tupple[1])\n",
    "    tokens_with_scores= [t_s for t_s in tokens_with_scores if t_s[1] > 0]\n",
    "    return tokens_with_scores\n",
    "\n",
    "    \n",
    "\n",
    "#return the IDF score of the word, 0 if didnt find\n",
    "def get_word_importance(word, index):\n",
    "    lex = index.getLexicon()\n",
    "\n",
    "    #Stats of the word from the index\n",
    "    lex_entry =  lex.getLexiconEntry(word.lower())\n",
    "    if lex_entry is None : \n",
    "        return 0\n",
    "    #calculating IDF\n",
    "    \n",
    "    N = index.getCollectionStatistics().getNumberOfDocuments()\n",
    "    n = lex_entry.getDocumentFrequency()\n",
    "    return math.log2(N / n)\n",
    "    \n",
    "   \n",
    "#return a tuple of all the tokens of the query question and all the enteties \n",
    "def get_query_tokens_and_enteties(question, index):\n",
    "    \n",
    "    doc = nlp(question) # tokkanization of the query, Stop words removal\n",
    "    entities = list(set([ent.text for ent in doc.ents]))\n",
    "   \n",
    "    important_terms = [\n",
    "        token.lemma_ for token in doc \n",
    "        if not token.is_stop and not token.is_punct and not token.i in entities and token.pos_ in [\"NOUN\", \"VERB\", \"PROPN\", \"ADJ\"]\n",
    "    ]\n",
    "    tokens_with_scores = sortByIDF(important_terms,index)\n",
    "    return (tokens_with_scores, entities)\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "@param: recive a query question \n",
    "reconize enteties, removing stop words sorting by idf score. \n",
    "@return: 2 lists, the first is a list of enteties, the second is a list of tockens sorted by the idf score. \n",
    "\"\"\"\n",
    "def process_query(query_question,index):\n",
    "    if not index:\n",
    "        try:\n",
    "            index = get_index(get_dataFrame())\n",
    "        except:\n",
    "            print(\"error while creating index\")\n",
    "            return\n",
    "    tockens_and_enteties = get_query_tokens_and_enteties(query_question, index)\n",
    "    return tockens_and_enteties\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([('INN', 8.063870878041314),\n",
       "  ('inform', 4.512710559843624),\n",
       "  ('friend', 3.361856287964306),\n",
       "  ('take', 2.769417891973364),\n",
       "  ('PUBLIC', 2.605503782633197)],\n",
       " ['FRIENDS', 'INN'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking query process \n",
    "query_question = \"Who informs his FRIENDS and the PUBLIC that he has taken that justly celebrated INN in this city\"\n",
    "process_query(query_question, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First stage retriver with BM25 \n",
    "bm25 = pt.terrier.Retriever(index, wmodel=\"BM25\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  qid   docid                     docno  rank      score   query\n",
      "0   1  122024      New_York_19190610_27     0  10.233243  Europe\n",
      "1   1   61883         Texas_18771215_13     1   9.759719  Europe\n",
      "2   1    9637      Maryland_18401110_20     2   9.730148  Europe\n",
      "3   1  110750      Nebraska_19020225_26     3   9.715430  Europe\n",
      "4   1   36539  Rhode_Island_18511030_25     4   9.642500  Europe\n",
      "5   1   47808      Illinois_18680907_40     5   9.578485  Europe\n",
      "6   1  100207         Kansas_18970902_5     6   9.570657  Europe\n",
      "7   1   65964     Tennessee_18740915_14     7   9.556417  Europe\n",
      "8   1   29371        Hawaii_18520522_33     8   9.509314  Europe\n",
      "9   1   38017     Tennessee_18590726_13     9   9.499877  Europe\n"
     ]
    }
   ],
   "source": [
    "#testing the bm25 engine \n",
    "query = \"Europe\"\n",
    "results = bm25.search(query)\n",
    "\n",
    "#show first 10 resaults\n",
    "print(results.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gettting the text of the document by its \"docno\"\n",
    "def get_text_by_docno(docno):\n",
    "    return df_documents.loc[docno]['text']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# hist word vectors embedding\n",
    "using hist word embadding to retrive the closest term to the query term for each decade from 1800 to 1920. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the root directory to hist_words\n",
    "import sys \n",
    "def init_HistWord():\n",
    "    status = True\n",
    "    path = os.getcwd()\n",
    "    home_dir = path.split(\"\\\\\")[-1]\n",
    "    if( home_dir == \"histwords_master\"): \n",
    "        os.chdir(\"..\") # changing root directory to the project root folder\n",
    "        print(f\"Working directory set to: {os.getcwd()}\")\n",
    "    project_home = os.path.abspath(\"histwords_master\")\n",
    "    if project_home not in sys.path:\n",
    "        sys.path.insert(0, project_home)\n",
    "    home_dir = path.split(\"\\\\\")[-1]\n",
    "    if( home_dir != \"histwords_master\"): \n",
    "        os.chdir(project_home)\n",
    "        print(f\"Working directory set to: {os.getcwd()}\")\n",
    "    try:\n",
    "        from representations.sequentialembedding import SequentialEmbedding\n",
    "        fiction_embeddings = SequentialEmbedding.load(\"embeddings/sgns\", list(range(1810, 2000, 10)))\n",
    "    except:\n",
    "        status = False\n",
    "    finally:\n",
    "        if(status): # if expansion passed \n",
    "            print(\"Successfuly loaded HistWord\")\n",
    "        else:\n",
    "            print(\"Failed to load HistWord\")\n",
    "            fiction_embeddings= None\n",
    "        path = os.getcwd()\n",
    "        home_dir = path.split(\"\\\\\")[-1]\n",
    "        if( home_dir == \"histwords_master\"): \n",
    "            os.chdir(\"..\") # changing root directory to the project root folder \n",
    "            print(f\"Working directory set to: {os.getcwd()}\")\n",
    "        print(type(fiction_embeddings))\n",
    "        return fiction_embeddings\n",
    "    \n",
    "def getSimilar_words(word, fiction_embeddings):\n",
    "    similar_words  = fiction_embeddings.get_seq_closest(word,1810, 110 )\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "home_dir = path.split(\"\\\\\")[-1]\n",
    "if( home_dir == \"histwords_master\"): \n",
    "    os.chdir(\"..\") # changing root directory to the original    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query with BM25 foreach word in the resault of the histWord similar words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#recive word and use histWord embedding to get similar words from diffrent times and then create the 1oo most relevant documents \n",
    "def bm25_simWords(query_word, fiction_embeddings):\n",
    "    maximum_length = 100\n",
    "    results = bm25.search(query_word).head(maximum_length)\n",
    "    similar_words = getSimilar_words(query_word, fiction_embeddings) #get the relevant similar words to the query word\n",
    "    for word in similar_words:\n",
    "        temp_bm25_resaults =  bm25.search(word).head(maximum_length)\n",
    "        results = pd.concat([results,temp_bm25_resaults] , ignore_index= True).drop_duplicates(subset='docid', keep='first')\n",
    "        results = results.sort_values(by = \"score\", ascending=False)\n",
    "        results = results.head(maximum_length)\n",
    "    results['rank'] = range(len(results))\n",
    "    return results\n",
    "#get the 100 top results of bm25 retrivial\n",
    "def bm25Search(word):\n",
    "     return bm25.search(word)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEes2xbJ0o4S"
   },
   "source": [
    "# Query process and first stage retrival\n",
    "The query process outputs a list of terms sorted by importance and a list of enteties.\n",
    "We will not use histWord embedding on Enteties but just on the terms of the query question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory set to: C:\\Users\\user\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\\histwords_master\n",
      "Successfuly loaded HistWord\n",
      "Working directory set to: C:\\Users\\user\\Desktop\\school\\3\\Bicoca\\InformationRetrival\\Project\n",
      "<class 'representations.sequentialembedding.SequentialEmbedding'>\n",
      "([('group', 7.797426419873223), ('crew', 6.948618782731673), ('team', 6.332476107945073), ('old', 2.8159433645977296)], [])\n",
      "concate temp df: \n",
      "    qid   docid                   docno  rank       score    query\n",
      "0    1  117038     Arizona_19100519_12     0  108.573532  [group]\n",
      "1    1  124690       Kansas_19160913_1     1  106.076879  [group]\n",
      "2    1    8127       Hawaii_18401024_4     2  105.427590  [group]\n",
      "3    1  110486    Michigan_19070621_19     3  104.797309  [group]\n",
      "4    1  101377     Wyoming_19000119_11     4  104.640914  [group]\n",
      "..  ..     ...                     ...   ...         ...      ...\n",
      "89   1   72593    Arkansas_18900305_13    95   81.063656  [group]\n",
      "90   1  130788   Tennessee_18580113_56    96   80.785634  [group]\n",
      "91   1   15062   Minnesota_18500227_10    97   80.745561  [group]\n",
      "92   1  115567       Alaska_19021004_3    98   80.628532  [group]\n",
      "93   1   98280  California_18961120_23    99   80.533237  [group]\n",
      "\n",
      "[100 rows x 6 columns] \n",
      " with the df: \n",
      "  Empty DataFrame\n",
      "Columns: [qid, docid, docno, rank, score, query]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_15616\\2747366670.py:40: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  bm25df = pd.concat([bm25df, temp_result], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concate temp df: \n",
      "    qid   docid                            docno  rank       score   query\n",
      "0    1   16609              Vermont_18450606_13     0  103.051406  [crew]\n",
      "1    1  120117                Oregon_19181111_6     1   96.603792  [crew]\n",
      "2    1  124879         Connecticut__19140522_11     2   90.279225  [crew]\n",
      "3    1   98899                Maine_18970626_25     3   89.759318  [crew]\n",
      "4    1   45689            Tennessee_18610704_15     4   89.662660  [crew]\n",
      "..  ..     ...                              ...   ...         ...     ...\n",
      "93   1   33623             Michigan_18600607_14    95   76.132201  [crew]\n",
      "94   1   99685          Connecticut__18960522_6    96   76.132201  [crew]\n",
      "95   1  109596  District_of_Columbia_19070324_4    97   76.066205  [crew]\n",
      "96   1  105766                  Iowa_19071121_6    98   75.946385  [crew]\n",
      "99   1  119159              Indiana_19160229_16    99   75.884919  [crew]\n",
      "\n",
      "[100 rows x 6 columns] \n",
      " with the df: \n",
      "       docid                   docno       score  qid    query\n",
      "71  117038     Arizona_19100519_12  108.573532  [1]  [group]\n",
      "94  124690       Kansas_19160913_1  106.076879  [1]  [group]\n",
      "1     8127       Hawaii_18401024_4  105.427590  [1]  [group]\n",
      "62  110486    Michigan_19070621_19  104.797309  [1]  [group]\n",
      "49  101377     Wyoming_19000119_11  104.640914  [1]  [group]\n",
      "..     ...                     ...         ...  ...      ...\n",
      "24   72593    Arkansas_18900305_13   81.063656  [1]  [group]\n",
      "98  130788   Tennessee_18580113_56   80.785634  [1]  [group]\n",
      "4    15062   Minnesota_18500227_10   80.745561  [1]  [group]\n",
      "64  115567       Alaska_19021004_3   80.628532  [1]  [group]\n",
      "45   98280  California_18961120_23   80.533237  [1]  [group]\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "concate temp df: \n",
      "    qid   docid                     docno  rank       score   query\n",
      "0    1   95641       Illinois_18951214_6     0  108.739318  [team]\n",
      "1    1   83975  Connecticut__18820203_16     1  104.008341  [team]\n",
      "2    1   57271  North_Dakota_18801231_24     2  103.441348  [team]\n",
      "3    1   55542   Mississippi_18800218_42     3  102.481999  [team]\n",
      "4    1  122620       Virginia_19120730_8     4  100.712567  [team]\n",
      "..  ..     ...                       ...   ...         ...     ...\n",
      "95   1   64360       Vermont_18721003_19    95   83.724436  [team]\n",
      "96   1  118370  South_Dakota_19130523_16    96   83.608568  [team]\n",
      "97   1   68210       Indiana_18800901_22    97   83.582957  [team]\n",
      "98   1  119345     New_Mexico_19150108_9    98   83.509141  [team]\n",
      "99   1  114859       Kentucky_19090331_5    99   83.502904  [team]\n",
      "\n",
      "[100 rows x 6 columns] \n",
      " with the df: \n",
      "        docid                             docno       score  qid          query\n",
      "197  131137               Maryland_18800911_5  171.508636  [1]  [crew, group]\n",
      "61    69440                Kansas_18770504_16  165.896528  [1]  [crew, group]\n",
      "148  117038               Arizona_19100519_12  108.573532  [1]        [group]\n",
      "185  124690                 Kansas_19160913_1  106.076879  [1]        [group]\n",
      "6      8127                 Hawaii_18401024_4  105.427590  [1]        [group]\n",
      "..      ...                               ...         ...  ...            ...\n",
      "100   96658               Arizona_18911112_17   83.867806  [1]         [crew]\n",
      "75    78229  District_of_Columbia_18870705_36   83.867806  [1]         [crew]\n",
      "97    93839               Alabama_18941204_27   83.812029  [1]         [crew]\n",
      "103   97636                  Texas_18950921_9   83.776323  [1]        [group]\n",
      "59    67672             New_Jersey_18800708_2   83.729333  [1]        [group]\n",
      "\n",
      "[100 rows x 5 columns]\n",
      "concate temp df: \n",
      "    qid   docid                      docno  rank      score  query\n",
      "0    1  103127   West_Virginia_18980805_7     0  30.350387  [old]\n",
      "1    1  103128   West_Virginia_18980805_9     1  30.219795  [old]\n",
      "2    1   30498  North_Carolina_18580728_9     2  29.871977  [old]\n",
      "3    1   14201       Virginia_18490319_16     3  29.019385  [old]\n",
      "4    1   88376    North_Dakota_18870408_8     4  28.404632  [old]\n",
      "..  ..     ...                        ...   ...        ...    ...\n",
      "82   1   29820       New_York_18540729_23    95  24.540746  [old]\n",
      "84   1    7200  South_Carolina_18361231_1    96  24.521157  [old]\n",
      "85   1   87752  West_Virginia_18830324_12    97  24.493177  [old]\n",
      "87   1  119089         Vermont_19160721_3    98  24.485544  [old]\n",
      "86   1   19454        Alabama_18451121_31    99  24.485544  [old]\n",
      "\n",
      "[100 rows x 6 columns] \n",
      " with the df: \n",
      "        docid                     docno       score  qid          query\n",
      "58    51244  Connecticut__18700519_16  172.360847  [1]  [team, group]\n",
      "198  131137       Maryland_18800911_5  171.508636  [1]  [crew, group]\n",
      "81    69440        Kansas_18770504_16  165.896528  [1]  [crew, group]\n",
      "113   95641       Illinois_18951214_6  108.739318  [1]         [team]\n",
      "156  117038       Arizona_19100519_12  108.573532  [1]        [group]\n",
      "..      ...                       ...         ...  ...            ...\n",
      "166  119992   North_Dakota_19110719_2   87.490395  [1]        [group]\n",
      "45    39680      Michigan_18600221_11   87.407979  [1]         [team]\n",
      "182  124333    California_19120519_13   87.350065  [1]         [crew]\n",
      "21    17890       Missouri_18490531_6   87.272605  [1]        [group]\n",
      "2      1567       Virginia_18170708_2   87.194642  [1]         [team]\n",
      "\n",
      "[100 rows x 5 columns]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>docno</th>\n",
       "      <th>score</th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>51244</td>\n",
       "      <td>Connecticut__18700519_16</td>\n",
       "      <td>172.360847</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[team, group]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>131137</td>\n",
       "      <td>Maryland_18800911_5</td>\n",
       "      <td>171.508636</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[crew, group]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>69440</td>\n",
       "      <td>Kansas_18770504_16</td>\n",
       "      <td>165.896528</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[crew, group]</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>95641</td>\n",
       "      <td>Illinois_18951214_6</td>\n",
       "      <td>108.739318</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[team]</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>117038</td>\n",
       "      <td>Arizona_19100519_12</td>\n",
       "      <td>108.573532</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[group]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>119992</td>\n",
       "      <td>North_Dakota_19110719_2</td>\n",
       "      <td>87.490395</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[group]</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>39680</td>\n",
       "      <td>Michigan_18600221_11</td>\n",
       "      <td>87.407979</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[team]</td>\n",
       "      <td>96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>124333</td>\n",
       "      <td>California_19120519_13</td>\n",
       "      <td>87.350065</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[crew]</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>17890</td>\n",
       "      <td>Missouri_18490531_6</td>\n",
       "      <td>87.272605</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[group]</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1567</td>\n",
       "      <td>Virginia_18170708_2</td>\n",
       "      <td>87.194642</td>\n",
       "      <td>[1]</td>\n",
       "      <td>[team]</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      docid                     docno       score  qid          query  rank\n",
       "64    51244  Connecticut__18700519_16  172.360847  [1]  [team, group]     0\n",
       "199  131137       Maryland_18800911_5  171.508636  [1]  [crew, group]     1\n",
       "95    69440        Kansas_18770504_16  165.896528  [1]  [crew, group]     2\n",
       "134   95641       Illinois_18951214_6  108.739318  [1]         [team]     3\n",
       "173  117038       Arizona_19100519_12  108.573532  [1]        [group]     4\n",
       "..      ...                       ...         ...  ...            ...   ...\n",
       "177  119992   North_Dakota_19110719_2   87.490395  [1]        [group]    95\n",
       "53    39680      Michigan_18600221_11   87.407979  [1]         [team]    96\n",
       "185  124333    California_19120519_13   87.350065  [1]         [crew]    97\n",
       "33    17890       Missouri_18490531_6   87.272605  [1]        [group]    98\n",
       "2      1567       Virginia_18170708_2   87.194642  [1]         [team]    99\n",
       "\n",
       "[100 rows x 6 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_question = \"\"\n",
    "fiction_embeddings = init_HistWord()\n",
    "def firstStageRetrival(query_question, fiction_embeddings ):   \n",
    "    terms_and_ent = process_query(query_question,index)\n",
    "    print(terms_and_ent)\n",
    "    terms = terms_and_ent[0] # terms of the query sorted by IDF score\n",
    "    entities = terms_and_ent[1] # entities from the query\n",
    "    #calculating total IDF score\n",
    "    total_IDF_score = 0\n",
    "    for t in terms:\n",
    "        total_IDF_score += t[1]\n",
    "    bm25df = pd.DataFrame(columns=['qid', 'docid', 'docno', 'rank', 'score', 'query'])\n",
    "    #get the 100 top resaults of the bm25 on the entities\n",
    "    maximum_length = 100\n",
    "    for ent in entities:\n",
    "        temp_bm25_resaults =  bm25Search(ent).head(maximum_length)\n",
    "        temp_bm25_resaults['query'] =[[ent]]* len(temp_bm25_resaults)\n",
    "        print(\"concate temp df: \\n\" , temp_bm25_resaults, \"\\n with the df: \\n \" , bm25df)\n",
    "        bm25df = pd.concat([bm25df,temp_bm25_resaults] , ignore_index= True)\n",
    "        bm25df = bm25df.groupby(['docid', 'docno'], as_index=False).agg({\n",
    "    'score': 'sum',\n",
    "    'qid': lambda x: list({q for sublist in x for q in sublist}),\n",
    "    'query': lambda x: list({q for sublist in x for q in sublist})\n",
    "})# aggrigate score field in already exsists documents in the results table \n",
    "        bm25df = bm25df.sort_values(by = \"score\", ascending=False)\n",
    "        bm25df = bm25df.head(maximum_length)\n",
    "    bm25df['rank'] = range(len(bm25df))\n",
    "    bm25df['score']  = bm25df['score']  *total_IDF_score# multipling all the score of the entities because we want to give more wight to entities than tokens\n",
    "    \n",
    "    #now run bm25 search on the tokens, documents that are already in the table are summed score\n",
    "    for term in terms:\n",
    "        temp_result =  bm25_simWords(term[0], fiction_embeddings) # get the top 100 relevant documents with bm25 \n",
    "        temp_result['score'] = temp_result['score'] * term[1] # weight the column by its IDF score\n",
    "        temp_result['query'] = [[term[0]]] * len(temp_result)  \n",
    "        for docid in temp_result['docid']:\n",
    "            if docid in bm25df['docid']:\n",
    "                print(\"found!!!!!!!!!!!!!!!!!\" , docid)\n",
    "        print(\"concate temp df: \\n\" , temp_result, \"\\n with the df: \\n \" , bm25df)\n",
    "        bm25df = pd.concat([bm25df, temp_result], ignore_index=True)\n",
    "        bm25df = bm25df.groupby(['docid', 'docno'], as_index=False).agg({\n",
    "    'score': 'sum',\n",
    "    'qid': lambda x: list({q for sublist in x for q in sublist}),\n",
    "    'query': lambda x: list({q for sublist in x for q in sublist})\n",
    "}) # aggrigate score field in already exsists documents in the results table \n",
    "        bm25df = bm25df.sort_values(by = \"score\", ascending=False)\n",
    "        bm25df = bm25df.head(maximum_length)\n",
    "    bm25df['rank'] = range(len(bm25df))\n",
    "    return bm25df\n",
    "    \n",
    "firstStageRetrival(query_question,fiction_embeddings)\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNUuc82OtGicqd8vHTH8YSN",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
